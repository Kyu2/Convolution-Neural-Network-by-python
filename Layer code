import numpy as np
from scipy import signal
import time

class FullyConnectedLayer(object):

	def __init__(self, input_dim, output_dim):

		self.weights = np.random.normal(scale=0.1, size=(input_dim,output_dim))
		self.bias = np.zeros(shape=output_dim)

	def forward(self, input):
		# start_time = time.time()
		x = input
		w = self.weights
		b = self.bias
		N = x.shape[0]
		D, M = w.shape
		output = x.reshape(N, D).dot(w) + b
		# duration = time.time() - start_time
		# print('FullyConnected forward took %.2f sec' % duration)

		return output

	def backward(self, grad_output, input):
		# start_time = time.time()
		dz = grad_output
		x = input
		w = self.weights
		b = self.bias
		N = x.shape[0]
		D, M = w.shape
		db = np.sum(dz, axis=0)
		dw = x.reshape(N, D).T.dot(dz)
		dx = dz.dot(w.T).reshape(x.shape)
		# duration = time.time() - start_time
		# print('FullyConnected backward took %.2f sec' % duration)

		return dx, dw, db

	def update_weights(self, grad_update):
		self.weights -= grad_update
		return

	def update_bias(self, grad_update):
		self.bias -= grad_update
		return


class ConvLayer(object):

	def __init__(self, input_dim, output_dim, kH, kW):

		self.weights = np.random.normal(scale=0.1, size=(output_dim,input_dim,kH,kW))
		self.bias = np.zeros(shape=output_dim)

	def forward(self, input):
		# start_time = time.time()
		x = input
		w = self.weights
		b = self.bias
		N, C, H, W = x.shape
		F, C, kH, kW = w.shape
		Ho = 1 + H - kH
		Wo = 1 + W - kW

		output = np.zeros((N, F, Ho, Wo))
		for n in xrange(N):
			for f in xrange(F):
				output[n, f] = signal.fftconvolve(x[n], w[f], 'valid')
		# duration = time.time() - start_time
		# print('Convolution forward took %.2f sec' % duration)

		return output

	def backward(self, grad_output, input):
		# start_time = time.time()
		dz = grad_output
		x = input
		w = self.weights
		b = self.bias
		N, F, Ho, Wo = dz.shape
		N, C, H, W = x.shape
		F, C, kH, kW = w.shape
		padH = (kH - 1) / 2
		padW = (kW - 1) / 2

		w_flip = np.zeros(w.shape)
		for f in xrange(F):
			for c in xrange(C):
				w_flip[f, c] = np.fliplr(np.flipud(w[f, c]))

		dx, dw, db = np.zeros(x.shape), np.zeros(w.shape), np.zeros(b.shape)
		db = np.sum(np.sum(np.sum(dz, axis=0),axis=1),axis=1)
		dz_pad = np.pad(dz, [(0,0), (0,0), (padH,padH), (padW,padW)], 'constant')

		for n in xrange(N):
			for f in xrange(F):
				for c in xrange(C):
					dx[n, c] += signal.fftconvolve(dz_pad[n, f], w_flip[f, c], 'same')
					dw[f, c] += signal.fftconvolve(x[n, c], dz[n, f], 'valid')
		# duration = time.time() - start_time
		# print('Convolution backward took %.2f sec' % duration)

		return dx, dw, db


	def update_weights(self, grad_update):
		self.weights -= grad_update
		return

	def update_bias(self, grad_update):
		self.bias -= grad_update
		return


class MaxPool(object):

	def __init__(self, pH, pW, stride):
		self.pH = pH
		self.pW = pW
		self.stride = stride

	def forward(self, input):
		x = input
		pH = self.pH
		pW = self.pW
		stride = self.stride
		N, C, H, W = x.shape
		Ho = H / stride
		Wo = W / stride

		output = x.reshape(N, C, Ho, stride, Wo, stride)
		output = np.amax(np.amax(output, axis=3), axis=4)

		output_reshape = output[:,:,:,np.newaxis,:,np.newaxis]*np.ones((1,1,1,stride,1,stride))
		output_reshape = output_reshape.reshape(x.shape)
		self.mask = (x == output_reshape)

		return output

	def backward(self, grad_output, input):
		dz = grad_output
		x = input
		stride = self.stride
		N, C, H, W = x.shape

		dz_reshape = dz[:,:,:,np.newaxis,:,np.newaxis]*np.ones((1,1,1,stride,1,stride))
		dz_reshape = dz_reshape.reshape(x.shape)
		dx = dz_reshape*self.mask

		return dx


class ReLU(object):

	def forward(self, input):
		output = input*(input>0)
		return output

	def backward(self, grad_output, input):
		grad_input = grad_output*(input>0)
		return grad_input


class PReLU(object):

	def __init__(self, input_dim):
		self.dim = input_dim
		self.a = np.ones(self.dim)*0.25

	def forward(self, input):
		C = self.dim
		a = self.a
		output = input
		output += (a.reshape(1,C,1,1)-1)*(input<0)*input
		return output

	def backward(self, grad_output, input):
		dz = grad_output
		x = input
		C = self.dim
		a = self.a
		dx = dz
		dx += (a.reshape(1,C,1,1)-1)*(input<0)*dz
		da = np.sum(np.sum(np.sum(dz*x*(x<0), axis=0), axis=1), axis=1)
		return dx, da

	def update_weights(self, grad_update):
		self.a -= grad_update
		return

	def update_bias(self, grad_update):
		return


class Dropout(object):

	def __init__(self, prob):
		self.mode = 'train'
		self.prob = prob

	def forward(self, input):
		x = input
		p = self.prob
		if self.mode == 'train':
			self.mask = np.random.binomial(1, p, size=x.shape) / p
			output = x*self.mask
		elif self.mode == 'test':
			output = x
		return output

	def backward(self, grad_output, input):
		dz = grad_output
		x = input
		if self.mode == 'train':
			dx = dz*self.mask
		elif self.mode == 'test':
			dx = dz
		return dz

	def set_mode(self, mode):
		self.mode = mode
		return


class SoftMax(object):

	def compute(self, output, label):
		X = output
		y = label
		N = X.shape[0]
		e = np.exp(X - np.max(X))
		det = np.sum(e, axis=1)
		probs = (e.T / det).T
		loss = -np.sum(np.log(probs)*y) / N
		dX = probs - y
		dX /= N
		return loss, dX
