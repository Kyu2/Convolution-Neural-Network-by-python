import numpy as np
import time

class Trainer(object):

	def __init__(self):

		self.learning_rate_init = 0.01
		self.learning_rate_decay = 1
		self.momentum = 0
		self.batch_size = 64
		self.num_epochs = 10

	def train(self, train_data, train_label, test_data, test_label,
			  model, config=None):

		self.model = model
		
		if config is not None:
			self.learning_rate_init = config['learning rate']
			self.learning_rate_decay = config['learning rate decay']
			self.momentum = config['momentum']
			self.batch_size = config['batch size']
			self.num_epochs = config['num epochs']
		self.grads_update = {}

		X = train_data
		y = train_label
		X_valid = test_data
		y_valid = test_label

		lr = self.learning_rate_init
		lrd = self.learning_rate_decay
		m = self.momentum

		N = X.shape[0]
		iter_per_epoch = N / self.batch_size
		num_iters = self.num_epochs * iter_per_epoch
		epoch = 0
		valid_acc_best = 0.0

		for epoch in xrange(self.num_epochs):

			batch_shuffle = np.arange(N)
			np.random.shuffle(batch_shuffle)

			for iter in xrange(iter_per_epoch):

				start_time = time.time()

				# random sample batch of data

				batch_index = iter*self.batch_size
				X_batch = X[batch_shuffle[batch_index:batch_index+self.batch_size]]
				y_batch = y[batch_shuffle[batch_index:batch_index+self.batch_size]]

				# compute loss and gradients

				loss, grads = self.model.backward(X_batch, y_batch)

				# update parameters

				for layer in model.layers:

					if not layer in self.grads_update:
						self.grads_update[layer] = {}
						for param in grads[layer]:
							self.grads_update[layer][param] = np.zeros(grads[layer][param].shape)

					for param in self.grads_update[layer]:
						self.grads_update[layer][param] = m * self.grads_update[layer][param] + lr * grads[layer][param]

				model.update_params(self.grads_update)

				duration = time.time() - start_time

				if iter and iter%10==0:
					print('epoch %d, iter %d / loss: %.4f, duration per iter: %.4f' % (epoch+1, iter, loss, duration))

				if iter and iter%100==0:
					start_time = time.time()
					model.set_mode('test')
					output_valid = model.forward(X_valid)
					pred_valid = np.argmax(output_valid, axis=1)
					label_valid = np.argmax(y_valid, axis=1)
					valid_acc = np.mean(pred_valid == label_valid)
					if valid_acc > valid_acc_best:
						valid_acc_best = valid_acc
					model.set_mode('train')
					duration = time.time() - start_time


					print('epoch %d, iter %d / loss %.4f, validation_accuracy: %.2f %s, validation took %.2f sec'
				  		  % (epoch+1, iter, loss, valid_acc*100, "%", duration))

			# end of each epoch

			# learning rate decay
			lr = lr*lrd

			start_time = time.time()
			model.set_mode('test')
			output_valid = model.forward(X_valid)
			pred_valid = np.argmax(output_valid, axis=1)
			label_valid = np.argmax(y_valid, axis=1)
			valid_acc = np.mean(pred_valid == label_valid)
			if valid_acc > valid_acc_best:
				valid_acc_best = valid_acc
			model.set_mode('train')
			duration = time.time() - start_time


			print('epoch %d finished / loss %.4f, validation_accuracy: %.2f %s'
				  % (epoch+1, loss, valid_acc*100, "%"))

	

		return valid_acc_best
